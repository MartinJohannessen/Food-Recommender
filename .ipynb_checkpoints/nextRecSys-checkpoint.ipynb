{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import SVD\n",
    "from surprise import Dataset\n",
    "from surprise import accuracy\n",
    "from surprise import Reader\n",
    "from surprise.model_selection import KFold\n",
    "from surprise.model_selection import cross_validate\n",
    "from surprise.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_profiles2 = pd.read_csv('../input/item-profiles2.csv', sep=';')\n",
    "item_profiles3 = pd.read_csv('../input/item-profiles3.csv', sep=';')\n",
    "ratings = pd.read_csv('../input/user-item-rating.csv', sep='\\t', names=['user_id','item_id','rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use this healthiness method to calculate the healthiness of recipes, and to set weights for the post filtering.\n",
    "def healthiness(itemsDataframe):\n",
    "    \n",
    "    # Calculate the energy percentage of each relevant macronutrient. 1g of fat contains 9 kCal. \n",
    "    fat = ((itemsDataframe['Fat (g)'] * 9) / itemsDataframe['Calories (kCal)']) * 100\n",
    "    sugar = ((itemsDataframe['Sugar (g)'] * 4) / itemsDataframe['Calories (kCal)']) * 100\n",
    "    saturatedFat = ((itemsDataframe['Saturated Fat (g)'] * 9) / itemsDataframe['Calories (kCal)']) * 100\n",
    "    \n",
    "    # This calculates a continous version of the healthiness score. \n",
    "    # Fat/3 because the recommended limit for fat is 3X the others. This \"unhealthiness\" metric was sadly not used.\n",
    "    itemsDataframe['Unhealtiness'] = (fat / 3) + sugar + saturatedFat\n",
    "    \n",
    "    # These are bounderies and points are tunable to influence \n",
    "    # post filter weights in accordence with the health recommendations.\n",
    "    itemsDataframe.loc[fat > 30, 'fatPoints'] = 0\n",
    "    itemsDataframe.loc[fat >= 40, 'fatPoints'] = -1\n",
    "    itemsDataframe.loc[fat >= 50, 'fatPoints'] = -2\n",
    "    itemsDataframe.loc[fat >= 60, 'fatPoints'] = -3\n",
    "    itemsDataframe.loc[fat >= 70, 'fatPoints'] = -4\n",
    "    itemsDataframe.loc[fat >= 80, 'fatPoints'] = -5\n",
    "    itemsDataframe.loc[fat <= 30, 'fatPoints'] = 1 \n",
    "    itemsDataframe.loc[fat <= 20, 'fatPoints'] = 2\n",
    "    itemsDataframe.loc[fat <= 10, 'fatPoints'] = 3\n",
    "    itemsDataframe.loc[fat <= 5, 'fatPoints'] = 4 \n",
    "    itemsDataframe.loc[fat <= 1, 'fatPoints'] = 5\n",
    "\n",
    "    itemsDataframe.loc[sugar > 10, 'sugarPoints'] = 0\n",
    "    itemsDataframe.loc[sugar >= 13, 'sugarPoints'] = -1\n",
    "    itemsDataframe.loc[sugar >= 16, 'sugarPoints'] = -2\n",
    "    itemsDataframe.loc[sugar >= 19, 'sugarPoints'] = -3\n",
    "    itemsDataframe.loc[sugar >= 22, 'sugarPoints'] = -4\n",
    "    itemsDataframe.loc[sugar >= 25, 'sugarPoints'] = -5\n",
    "    itemsDataframe.loc[sugar <= 10, 'sugarPoints'] = 1 \n",
    "    itemsDataframe.loc[sugar <= 7, 'sugarPoints'] = 2\n",
    "    itemsDataframe.loc[sugar <= 5, 'sugarPoints'] = 3 \n",
    "    itemsDataframe.loc[sugar <= 3, 'sugarPoints'] = 4\n",
    "    itemsDataframe.loc[sugar <= 1, 'sugarPoints'] = 5 \n",
    "        \n",
    "    itemsDataframe.loc[saturatedFat > 10, 'satFatPoints'] = 0\n",
    "    itemsDataframe.loc[saturatedFat >= 13, 'satFatPoints'] = -1\n",
    "    itemsDataframe.loc[saturatedFat >= 16, 'satFatPoints'] = -2\n",
    "    itemsDataframe.loc[saturatedFat >= 19, 'satFatPoints'] = -3\n",
    "    itemsDataframe.loc[saturatedFat >= 22, 'satFatPoints'] = -4\n",
    "    itemsDataframe.loc[saturatedFat >= 25, 'satFatPoints'] = -5\n",
    "    itemsDataframe.loc[saturatedFat <= 10, 'satFatPoints'] = 1 \n",
    "    itemsDataframe.loc[saturatedFat <= 7, 'satFatPoints'] = 2\n",
    "    itemsDataframe.loc[saturatedFat <= 5, 'satFatPoints'] = 3 \n",
    "    itemsDataframe.loc[saturatedFat <= 3, 'satFatPoints'] = 4\n",
    "    itemsDataframe.loc[saturatedFat <= 1, 'satFatPoints'] = 5 \n",
    "        \n",
    "       \n",
    "    itemsDataframe['Healthiness'] = itemsDataframe['fatPoints'] + itemsDataframe['satFatPoints'] + itemsDataframe['sugarPoints']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply healthiness evaluation to each recipe in item_profiles2\n",
    "healthiness(item_profiles2)\n",
    "\n",
    "# Put healthiness as value in a dictionary with recipe as key.\n",
    "healthiness_profiles = item_profiles2[['Recipe ID', 'Healthiness']]\n",
    "healthiness_dict = defaultdict()\n",
    "for _,Recipe_ID, Healthiness in healthiness_profiles.itertuples():\n",
    "    healthiness_dict[Recipe_ID] = Healthiness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This is the post filter method. Suprise predictions are given in a list of immutable tuples.\n",
    "# To post filter the predictions I make a dataframe of the predictions list, change the predictions,\n",
    "# and make a new list of tuples.\n",
    "def postfilter(predictions, healthinessFactor):\n",
    "    ratingsProcessed = pd.DataFrame.from_records(predictions, columns=['uid', 'iid', 'r_ui', 'est', 'details'])\n",
    "    ratingsProcessed = ratingsProcessed.join(item_profiles2['Healthiness'], how= 'inner', on= 'iid', sort=False)\n",
    "    # This is were the values are transformed based on the healthinessFactor parameter and the health points of the recipes.\n",
    "    ratingsProcessed['est'] = ratingsProcessed['est'] + (ratingsProcessed['Healthiness']*healthinessFactor)\n",
    "    ratingsProcessed.drop(labels='Healthiness', axis=1, inplace=True)\n",
    "    ratingsProcessed = list(ratingsProcessed.itertuples(name='Prediction', index=False))\n",
    "    return ratingsProcessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from sklearn.metrics import ndcg_score\n",
    "\n",
    "# We used Suprise for much of this code https://surprise.readthedocs.io/en/stable/FAQ.html \n",
    "def evaluations_at_k(predictions):\n",
    "\n",
    "    # K is the number of highest ranking predictions to consider\n",
    "    k = 10\n",
    "    # This is the threshold for what is considered an adequate recommendation.\n",
    "    threshold = 4\n",
    "    \n",
    "    # We map the predictions to the users. uid=user identity, iid=item identity, true_r[ating], est[imated rating]. \n",
    "    top_n = defaultdict(list)\n",
    "    for uid, iid, true_r, est, _ in predictions:\n",
    "        top_n[uid].append((iid, est))\n",
    "\n",
    "    # Sort the predictions for each user and retrieve the k highest ones and put them in a dictionary.\n",
    "    for uid, user_ratings in top_n.items():\n",
    "        user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_n[uid] = user_ratings[:k]\n",
    "        \n",
    "    # Remove the dictionary entries that didn't have enough ratings because of the data partitioning. \n",
    "    top_k = top_n.copy()\n",
    "    for uid, user_ratings in top_k.items():\n",
    "        if (len(top_k[uid]) != k):\n",
    "            top_n.pop(uid)\n",
    "            \n",
    "    # Calculate the healthiness average of all recommendations. \n",
    "    # 0 = the tipping point between inside and outside of HDR recommendations. Negative value means unhealthy.\n",
    "    healthinessAverage = 0\n",
    "    count = 0\n",
    "    for x in top_n.items():\n",
    "        count =  count + 1\n",
    "        healthiness = 0\n",
    "        for y in x[1]:\n",
    "            #healthiness = healthiness + healthiness_dict[y[0]] # This should work\n",
    "            healthiness = healthiness + item_profiles2.at[y[0], 'Healthiness'] # This works, but shouldn't work\n",
    "        healthinessAverage = healthinessAverage + (healthiness/k)\n",
    "    \n",
    "    healthinessAverage = healthinessAverage/count\n",
    "    \n",
    "    # This time we put put the estimated rating and the true rating in the dictionary.\n",
    "    user_est_true = defaultdict(list)\n",
    "    for uid, _, true_r, est, _ in predictions:\n",
    "        user_est_true[uid].append((est, true_r))\n",
    "\n",
    "    precisions = dict()\n",
    "    recalls = dict()\n",
    "    for uid, user_ratings in user_est_true.items():\n",
    "        \n",
    "        # Sort user ratings by estimated value\n",
    "        user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "        # Number of relevant items\n",
    "        n_rel = sum((true_r >= threshold) for (_, true_r) in user_ratings)\n",
    "\n",
    "        # Number of recommended items in top k\n",
    "        n_rec_k = sum((est >= threshold) for (est, _) in user_ratings[:k])\n",
    "\n",
    "        # Number of relevant and recommended items in top k\n",
    "        n_rel_and_rec_k = sum(((true_r >= threshold) and (est >= threshold))\n",
    "                              for (est, true_r) in user_ratings[:k])\n",
    "\n",
    "        # Precision@K: Proportion of recommended items that are relevant\n",
    "        # When n_rec_k is 0, Precision is undefined. We here set it to 0.\n",
    "\n",
    "        precisions[uid] = n_rel_and_rec_k / n_rec_k if n_rec_k != 0 else 0\n",
    "\n",
    "        # Recall@K: Proportion of relevant items that are recommended\n",
    "        # When n_rel is 0, Recall is undefined. We here set it to 0.\n",
    "\n",
    "        recalls[uid] = n_rel_and_rec_k / n_rel if n_rel != 0 else 0\n",
    "        \n",
    "    # Calculate the averages of precision and recall\n",
    "    precision = (sum(prec for prec in precisions.values())) / len(precisions)\n",
    "    recall = (sum(rec for rec in recalls.values())) / len(recalls)\n",
    "    \n",
    "    #MAE \n",
    "    from sklearn.metrics import mean_absolute_error\n",
    "    y_true = [[x[2] for x in predictions]]\n",
    "    y_pred = [[x[3] for x in predictions]]\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "\n",
    "    #NDCG\n",
    "    ndcg = ndcg_score(y_true, y_pred, k)\n",
    "\n",
    "    return precision, recall, ndcg, healthinessAverage, mae\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_svd_estimates(train_ratings):   \n",
    "    # We extract the user_id and recipe_id series and cross join them such that each unique pairing gets a spot.\n",
    "    users = train_ratings['user_id'].drop_duplicates().to_frame()\n",
    "    recipes = train_ratings['recipe_id'].drop_duplicates().to_frame()\n",
    "    users['key'] = 1\n",
    "    recipes['key'] = 1\n",
    "    users_recipes = users.merge(recipes, how='outer', on=['key']).drop(\"key\", 1)\n",
    "    \n",
    "    # Fit the original train ratings.\n",
    "    reader = Reader()\n",
    "    data = Dataset.load_from_df(train_ratings, reader)\n",
    "    trainset = data.build_full_trainset()\n",
    "    svd.fit(trainset)\n",
    "\n",
    "    # Put the true ratings back in train ratings.\n",
    "    train_ratings = pd.merge(train_ratings, users_recipes, how='outer', on=['user_id', 'recipe_id'])\n",
    "\n",
    "    # Estimate the ratings where there are no true rating.\n",
    "    foo = train_ratings[train_ratings['rating'].isna()].apply(lambda x: svd.predict(x.user_id, x.recipe_id, r_ui=None)[3], axis=1)\n",
    "    train_ratings.loc[train_ratings['rating'].isna(),'rating'] = foo\n",
    "    \n",
    "    return train_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining our content based algorithm based on Feyneand Bekocsky \n",
    "def content_based_predict(train, test):\n",
    "    \n",
    "    train_ratings = ratings.rename(columns={'item_id': 'recipe_id'}).iloc[train]\n",
    "    \n",
    "    # If this line is not commented out, a call to content_based_predict will be a hybrid approach.\n",
    "    #\n",
    "    #train_ratings = input_svd_estimates(train_ratings)\n",
    "    #\n",
    "\n",
    "    # Arange the data\n",
    "    recipe_id_ingredient_id = item_profiles3[['Recipe ID', 'Ingredient ID']]\n",
    "    ingredientRatings = train_ratings.merge(recipe_id_ingredient_id, left_on='recipe_id', right_on='Recipe ID').drop('Recipe ID', 1)\n",
    "    ingredientRatings = ingredientRatings[['user_id', 'recipe_id', 'Ingredient ID', 'rating']]\n",
    "    ingredientRatings = ingredientRatings.rename(columns={'Ingredient ID': 'ingredient_id', 'item_id': 'recipe_id', 'rating': 'rat'})\n",
    "\n",
    "    # Fit the data\n",
    "    recipes_dict = defaultdict(list) \n",
    "    ingredients_dict = defaultdict(lambda: defaultdict(list))  \n",
    "    for user_id, recipe_id, ingredient_id, rating in ingredientRatings.itertuples(name='rating', index=False):\n",
    "        recipes_dict[recipe_id].append(ingredient_id) \n",
    "        ingredients_dict[user_id][ingredient_id].append(rating)\n",
    "    for user_id in ingredients_dict.copy().keys():\n",
    "        for ingredient_id in ingredients_dict[user_id].copy().keys():\n",
    "            # Replace the lists in the leaf nodes of this nested dict with the average of the ingredient scores for the ingredient.\n",
    "            ingredients_dict[user_id][ingredient_id]  = sum(ingredients_dict[user_id][ingredient_id])/len(ingredients_dict[user_id][ingredient_id])\n",
    "    \n",
    "\n",
    "    # Load the test set.\n",
    "    test_ratings = ratings.rename(columns={'item_id': 'recipe_id'}).iloc[test]\n",
    "    \n",
    "    # predict the data\n",
    "    content_based_predictions = defaultdict(lambda: defaultdict())                        \n",
    "    for user_id, recipe_id, _ in test_ratings.itertuples(name='predictions', index=False):\n",
    "        rating_est = 0.0\n",
    "        counter = 0\n",
    "        for ingredient_id in recipes_dict[recipe_id]:\n",
    "            if(type(ingredients_dict[user_id][ingredient_id]) == float):\n",
    "                rating_est += ingredients_dict[user_id][ingredient_id]\n",
    "                counter += 1\n",
    "        if(counter != 0):\n",
    "            content_based_predictions[user_id][recipe_id] = rating_est/counter\n",
    "        else:\n",
    "            content_based_predictions[user_id][recipe_id] = np.nan\n",
    "    \n",
    "    # Use the above nested loop as a lambda function to prepare the predictions for evaluation.\n",
    "    test_ratings['rating_est'] = test_ratings.apply(lambda x: content_based_predictions[x.user_id][x.recipe_id], axis=1)\n",
    "    test_ratings['details'] = \"{'is it awesome': YES}\"\n",
    "    test_ratings.columns = ['uid', 'iid', 'r_ui', 'est', 'details']\n",
    "    test_ratings = test_ratings.dropna()\n",
    "    predictions = list(test_ratings.itertuples(name='Prediction', index=False))\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create random baseline.\n",
    "import random\n",
    "def get_baseline(predictions):\n",
    "    predDF = pd.DataFrame.from_records(predictions, columns=['uid', 'iid', 'r_ui', 'est', 'details'])\n",
    "    predDF['est'] = np.random.randint(1, 6, predDF.shape[0])\n",
    "    randomized_predictions = list(predDF.itertuples(index=False))\n",
    "    return randomized_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use grid search cross validation to tune parameters for the SVD.\n",
    "reader = Reader()\n",
    "data = Dataset.load_from_df(ratings, reader)\n",
    "param_grid = {'n_epochs': [5,5], 'lr_all': [0.01, 0.1], 'reg_all':[0.01,0.1]}\n",
    "grid_search = GridSearchCV(SVD, param_grid, measures=['rmse'], cv=2)\n",
    "grid_search.fit(data)\n",
    "\n",
    "print(grid_search.best_params['rmse'])\n",
    "svd = grid_search.best_estimator['rmse']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use ordinary k fold for the SVD.\n",
    "kf = KFold(n_splits=5)\n",
    "\n",
    "# Just to print the evaluation metrics all at once.\n",
    "precisionAvg = 0\n",
    "recallAvg = 0\n",
    "ndcgAvg = 0\n",
    "healthinessAvg = 0\n",
    "maeS = 0\n",
    "for trainset, testset in kf.split(data):\n",
    "    svd.fit(trainset)\n",
    "    # The second parameter is the post fileter healthiness factor.\n",
    "    predictions2 = postfilter(svd.test(testset), 0.0)\n",
    "    randomized_predictions = get_baseline(predictions2) \n",
    "    # evaluate predictions.\n",
    "    precision, recall, ndcg, healthinessAverage, mae = evaluations_at_k(predictions2)\n",
    "    #precision, recall, ndcg, healthinessAverage, mae = evaluations_at_k(randomized_predictions) #vs predictions2 randomized_predictions\n",
    "    precisionAvg = precisionAvg + precision\n",
    "    recallAvg = recallAvg + recall\n",
    "    ndcgAvg = ndcgAvg + ndcg\n",
    "    healthinessAvg = healthinessAvg + healthinessAverage\n",
    "    maeS = mae + maeS\n",
    "\n",
    "print(\"Precision:\", precisionAvg/5, \"Recall:\", recallAvg/5, \"NDCG_score:\", ndcgAvg/5, \"Healthiness:\", healthinessAvg/5, 'MAE:', maeS/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We must use stratified splits for the content based algoritm\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1) \n",
    "\n",
    "precisionAvgr = 0\n",
    "recallAvgr = 0\n",
    "ndcgAvgr = 0\n",
    "healthinessAvgr = 0\n",
    "maeAvgr = 0\n",
    "# Run the algorithm 5 times and evaluate.\n",
    "for train, test in skf.split(ratings, ratings['rating']):\n",
    "    predictions1 = content_based_predict(train, test)\n",
    "    # The postfilter is working, but the healthiness score has broken down. And time! Luckily we got out data.\n",
    "    predictions1 = postfilter(predictions1, 0.2)\n",
    "    #randomized_predictions = get_baseline(predictions1) \n",
    "    precision, recall, ndcg, healthinessAverage, mae = evaluations_at_k(predictions1)\n",
    "    precisionAvgr = precisionAvgr + precision\n",
    "    recallAvgr = recallAvgr + recall\n",
    "    ndcgAvgr = ndcgAvgr + ndcg\n",
    "    healthinessAvgr = healthinessAvgr + healthinessAverage\n",
    "    maeAvgr = mae + maeAvgr\n",
    "    print(\"Precision:\", precision, \"Recall:\", recall, \"NDCG_score:\", ndcg, \"Healthiness:\", healthinessAverage, \"MAE:\", mae)\n",
    "   \n",
    "print(\"Precision:\", precisionAvgr/5, \"Recall:\", recallAvgr/5, \"NDCG_score:\", ndcgAvgr/5, \"Healthiness:\", healthinessAvgr/5, \"MAE:\", maeAvgr/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the reason why we think the recall at 10 score is also broken. \n",
    "foo = ratings[ratings['rating'] > 3].groupby('user_id')['rating'].count()\n",
    "foo = 10/foo\n",
    "foo.mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
